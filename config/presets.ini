version = 1

[*]
threads = 16
flash-attn = on
cache-type-k = q8_0
cache-type-v = q8_0
n-gpu-layers = 999
parallel = 1
models-max = 1

[Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL]
ctx-size = 262144
jinja = true
temp = 0.15
min-p = 0.01

[GLM-4.6V-Flash-Q8_0]
ctx-size = 131072
jinja = true
temp = 0.8
top-k = 2
top-p = 0.6
repeat-penalty = 1.1

[gpt-oss-20b-mxfp4]
ctx-size = 131072
temp = 1.0
top-k = 0
top-p = 1.0

[Nemotron-3-Nano-30B-A3B-Q5_K_S]
no-kv-offload = true
ctx-size = 262144
temp = 1.0
top-p = 1.0

[Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL]
no-kv-offload = true
ctx-size = 262144
temp = 0.7
top-k = 20
min-p = 0.00
top-p = 0.8
repeat-penalty = 1.05
